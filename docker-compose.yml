services:
  ollama-vulkan:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # Update this to the latest Ollama version
        OLLAMA_VERSION: ${OLLAMA_VERSION:-v0.12.6}
    container_name: ollama-vulkan-arc
    restart: unless-stopped
    
    # Map Intel Arc GPU into container
    devices:
      - /dev/dri:/dev/dri
    
    # Ports
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    
    # Volumes for persistent model storage
    volumes:
      - ollama-models:/root/.ollama
    
    # Environment variables for Intel Arc A770
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0:11434}
      - OLLAMA_NUM_GPU=${OLLAMA_NUM_GPU:-999}
      - OLLAMA_NUM_CTX=${OLLAMA_NUM_CTX:-16384}
      - ZES_ENABLE_SYSMAN=${ZES_ENABLE_SYSMAN:-1}
      - ONEAPI_DEVICE_SELECTOR=${ONEAPI_DEVICE_SELECTOR:-}
    
    # Resource limits (adjust based on your system)
    mem_limit: ${MEMORY_LIMIT:-32g}
    shm_size: ${SHARED_MEMORY:-16g}
    
    # Healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Optional: Open WebUI for easy interaction
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    profiles:
      - webui
    ports:
      - "${WEBUI_PORT:-8080}:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama-vulkan:11434
    depends_on:
      ollama-vulkan:
        condition: service_healthy

volumes:
  ollama-models:
    driver: local
  open-webui:
    driver: local